{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_eightlayer_tri.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ORGG7XZloae",
        "colab_type": "code",
        "outputId": "1ce3cf84-aa73-4c62-f136-a5aeebb97b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDvfpsGLmO1_",
        "colab_type": "code",
        "outputId": "5565902a-0a14-412d-aaa4-e876aab25569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import sys\n",
        "# model file directory\n",
        "sys.path.append('/content/drive/My Drive/pymodule')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import librosa.display\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "from os.path import join\n",
        "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from sklearn import metrics \n",
        "\n",
        "from cnn8layer import Cnn8layer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQp_TCgBCvmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RANDOM_STATE = 42\n",
        "num_channels = 1\n",
        "num_rows = 128\n",
        "num_columns = 20\n",
        "num_labels = 3\n",
        "checkpointer = '/content/drive/My Drive/makinter_checkpoint/8layer_tri_v2_0131.hdf5'\n",
        "#output_dir = '/content/drive/My Drive/makinter_checkpoint/8layer.hdf5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN44Kz6UDYN4",
        "colab_type": "code",
        "outputId": "8b521c67-1475-417d-9004-c6ff44f1db98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# train test split\n",
        "datapath = '/content/drive/My Drive/Kickboard_Software/real_data'\n",
        "\n",
        "as_data = '/content/drive/My Drive/Kickboard_Software/real_data/as'\n",
        "ce_data = '/content/drive/My Drive/Kickboard_Software/real_data/ce'\n",
        "ur_data = '/content/drive/My Drive/Kickboard_Software/real_data/ur'\n",
        "\n",
        "as_arr_data = '/content/drive/My Drive/Kickboard_Software/real_data/as/arr'\n",
        "ce_arr_data = '/content/drive/My Drive/Kickboard_Software/real_data/ce/arr'\n",
        "ur_arr_data = '/content/drive/My Drive/Kickboard_Software/real_data/ur/arr'\n",
        "\n",
        "as_train_dir = '/content/drive/My Drive/Kickboard_Software/real_data/train/as'\n",
        "ce_train_dir = '/content/drive/My Drive/Kickboard_Software/real_data/train/ce'\n",
        "ur_train_dir = '/content/drive/My Drive/Kickboard_Software/real_data/train/ur'\n",
        "\n",
        "as_test_dir = '/content/drive/My Drive/Kickboard_Software/real_data/test/as'\n",
        "ce_test_dir = '/content/drive/My Drive/Kickboard_Software/real_data/test/ce'\n",
        "ur_test_dir = '/content/drive/My Drive/Kickboard_Software/real_data/test/ur'\n",
        "\n",
        "as_files = [f for f in listdir(as_data) if isfile(join(as_data,f))]\n",
        "as_train, as_test = train_test_split(as_files, test_size=0.1, random_state = RANDOM_STATE)\n",
        "print(\"as train\")\n",
        "print(len(as_train))\n",
        "print(\"as test\")\n",
        "print(len(as_test))\n",
        "\n",
        "for i in tqdm(range(len(as_test))):\n",
        "  shutil.move(join(as_data, as_test[i]), join(as_test_dir, as_test[i])) \n",
        "\n",
        "ce_files = [f for f in listdir(ce_data) if isfile(join(ce_data,f))]\n",
        "ce_train, ce_test = train_test_split(ce_files, test_size=0.1, random_state = RANDOM_STATE)\n",
        "print(\"ce train\")\n",
        "print(len(ce_train))\n",
        "print(\"ce test\")\n",
        "print(len(ce_test))\n",
        "\n",
        "for i in tqdm(range(len(ce_test))):\n",
        "  shutil.move(join(ce_data, ce_test[i]), join(ce_test_dir, ce_test[i])) \n",
        "\n",
        "ur_files = [f for f in listdir(ur_data) if isfile(join(ur_data,f))]\n",
        "ur_train, ur_test = train_test_split(ur_files, test_size=0.1, random_state = RANDOM_STATE)\n",
        "print(\"ur train\")\n",
        "print(len(ur_train))\n",
        "print(\"ur test\")\n",
        "print(len(ur_test))\n",
        "\n",
        "for i in tqdm(range(len(ur_test))):\n",
        "  shutil.move(join(ur_data, ur_test[i]), join(ur_test_dir, ur_test[i])) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/600 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "as train\n",
            "5400\n",
            "as test\n",
            "600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [00:04<00:00, 141.10it/s]\n",
            "  0%|          | 0/570 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ce train\n",
            "5130\n",
            "ce test\n",
            "570\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 570/570 [00:05<00:00, 113.34it/s]\n",
            "  0%|          | 0/606 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ur train\n",
            "5454\n",
            "ur test\n",
            "606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 606/606 [00:05<00:00, 113.36it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r86NK7aERqa1",
        "colab_type": "code",
        "outputId": "51f9a42c-a793-47f4-cc36-04a32b54a66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# create train data\n",
        "as_data = '/content/drive/My Drive/Kickboard_Software/real_data/as'\n",
        "ce_data = '/content/drive/My Drive/Kickboard_Software/real_data/ce'\n",
        "ur_data = '/content/drive/My Drive/Kickboard_Software/real_data/ur'\n",
        "\n",
        "as_train = [f for f in listdir(as_data) if isfile(join(as_data,f))]\n",
        "print(len(as_train))                                                    \n",
        "\n",
        "ce_train = [f for f in listdir(ce_data) if isfile(join(ce_data,f))]\n",
        "print(len(ce_train))                                                    \n",
        "\n",
        "ur_train = [f for f in listdir(ur_data) if isfile(join(ur_data,f))]\n",
        "print(len(ur_train))                                                    \n",
        "\n",
        "# noise testing\n",
        "\"\"\"\n",
        "noise = np.random.normal(0,30,10000)\n",
        "print(noise)\n",
        "noise = np.random.normal(0,30,10000)\n",
        "print(noise)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "noise = np.random.normal(0,30,10000)\n",
        "div = (np.max(noise) - np.min(noise))/2.\n",
        "if div == 0:\n",
        "    div = 1.\n",
        "\n",
        "noise = noise / div\n",
        "noise = noise - noise.mean(axis=0)\n",
        "noise_S = librosa.feature.melspectrogram(y=noise,sr=10000)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "S_DB = librosa.power_to_db(S, ref=np.max)\n",
        "librosa.display.specshow(S_DB, sr=10000, hop_length=512, x_axis='time', y_axis='mel');\n",
        "plt.colorbar(format='%+2.0f dB');\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "feat = np.load(join(as_data,as_train[0]))\n",
        "S_DB = librosa.power_to_db(feat, ref=np.max)\n",
        "librosa.display.specshow(S_DB, sr=10000, hop_length=512, x_axis='time', y_axis='mel');\n",
        "plt.colorbar(format='%+2.0f dB');\n",
        "\"\"\"\n",
        "dataset_x = []\n",
        "dataset_y = []\n",
        "\n",
        "as_cnt = 0\n",
        "print(\"loading features for asphalt\")\n",
        "for feat_file in tqdm(as_train):\n",
        "  try:\n",
        "    feat = np.load(join(as_data, feat_file))\n",
        "    dataset_x.append(feat)\n",
        "    dataset_y.append(1)\n",
        "    as_cnt += 1\n",
        "    \"\"\"\n",
        "    for i in range(5):\n",
        "      noise = np.random.normal(0,30,10000)\n",
        "      div = (np.max(noise) - np.min(noise))/2.\n",
        "      if div == 0:\n",
        "          div = 1.\n",
        "      noise = noise / div\n",
        "      noise = noise - noise.mean(axis=0)\n",
        "      noise_S = librosa.feature.melspectrogram(y=noise,sr=10000)\n",
        "      noise_added = feat + noise_S*0.01\n",
        "      dataset_x.append(noise_added)\n",
        "      dataset_y.append(1)\n",
        "      as_cnt += 1\n",
        "    \"\"\"\n",
        "    \n",
        "  except:\n",
        "    print(\"unable to load\")\n",
        "    pass\n",
        "print(\"finished loading %d features for asphalt\" % as_cnt)\n",
        "\n",
        "ce_cnt = 0\n",
        "print(\"loading features for ce\")\n",
        "for feat_file in tqdm(ce_train):\n",
        "  try:\n",
        "    feat = np.load(join(ce_data, feat_file))\n",
        "    dataset_x.append(feat)\n",
        "    dataset_y.append(0)\n",
        "    ce_cnt += 1\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(5):\n",
        "      noise = np.random.normal(0,30,10000)\n",
        "      div = (np.max(noise) - np.min(noise))/2.\n",
        "      if div == 0:\n",
        "          div = 1.\n",
        "      noise = noise / div\n",
        "      noise = noise - noise.mean(axis=0)\n",
        "      noise_S = librosa.feature.melspectrogram(y=noise,sr=10000)\n",
        "      noise_added = feat + noise_S*0.01\n",
        "      dataset_x.append(noise_added)\n",
        "      dataset_y.append(0)\n",
        "      ce_cnt += 1\n",
        "    \"\"\"\n",
        "  except:\n",
        "    print(\"unable to load\")\n",
        "    pass\n",
        "print(\"finished loading %d features for ce\" % ce_cnt)\n",
        "\n",
        "ur_cnt = 0\n",
        "print(\"loading features for ur\")\n",
        "for feat_file in tqdm(ur_train):\n",
        "  try:\n",
        "    feat = np.load(join(ur_data, feat_file))\n",
        "    dataset_x.append(feat)\n",
        "    dataset_y.append(2)\n",
        "    ur_cnt += 1\n",
        "\n",
        "    \"\"\"\n",
        "    for i in range(5):\n",
        "      noise = np.random.normal(0,30,10000)\n",
        "      div = (np.max(noise) - np.min(noise))/2.\n",
        "      if div == 0:\n",
        "          div = 1.\n",
        "      noise = noise / div\n",
        "      noise = noise - noise.mean(axis=0)\n",
        "      noise_S = librosa.feature.melspectrogram(y=noise,sr=10000)\n",
        "      noise_added = feat + noise_S*0.01\n",
        "      dataset_x.append(noise_added)\n",
        "      dataset_y.append(2)\n",
        "      ur_cnt += 1\n",
        "    \"\"\"\n",
        "  except:\n",
        "    print(\"unable to load\")\n",
        "    pass\n",
        "print(\"finished loading %d features for ur\" % ur_cnt)\n",
        "\n",
        "x = np.array(dataset_x)\n",
        "y = np.array(dataset_y)\n",
        "le = LabelEncoder()\n",
        "yy = to_categorical(le.fit_transform(y))\n",
        "\n",
        "_x_train, __x, _y_train, __y = train_test_split(x,yy,test_size=0.001, random_state = RANDOM_STATE)\n",
        "print(\"len train x: \", len(_x_train))\n",
        "print(\"len train y: \", len(_y_train))\n",
        "_x_train = _x_train.reshape(_x_train.shape[0],num_rows, num_columns, num_channels)\n",
        "\n",
        "np.save('/content/drive/My Drive/Kickboard_Software/_x_train_v2_0131',_x_train)\n",
        "np.save('/content/drive/My Drive/Kickboard_Software/_y_train_v2_0131',_y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5940\n",
            "5970\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5940 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5934\n",
            "loading features for asphalt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5940/5940 [20:50<00:00,  5.75it/s]\n",
            "  0%|          | 0/5970 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finished loading 5940 features for asphalt\n",
            "loading features for ce\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5970/5970 [19:41<00:00,  4.94it/s]\n",
            "  0%|          | 0/5934 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finished loading 5970 features for ce\n",
            "loading features for ur\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5934/5934 [20:34<00:00,  2.36it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finished loading 5934 features for ur\n",
            "len train x:  17826\n",
            "len train y:  17826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKxt2fLj6Fu5",
        "colab_type": "code",
        "outputId": "1cb72ce5-ebda-49ca-bb4d-847276886ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "#create test data\n",
        "\n",
        "as_test_dir = '/content/drive/My Drive/Kickboard_Software/real_data/test/as'\n",
        "ce_test_dir = '/content/drive/My Drive/Kickboard_Software/real_data/test/ce'\n",
        "ur_test_dir = '/content/drive/My Drive/Kickboard_Software/real_data/test/ur'\n",
        "\n",
        "dataset_x = []\n",
        "dataset_y = []\n",
        "\n",
        "ce_files = os.listdir(ce_test_dir)\n",
        "ce_cnt = 0\n",
        "print(\"loading test features for cement\")\n",
        "for feat_file in tqdm(ce_files):\n",
        "    try:\n",
        "      feat = np.load(join(ce_test_dir, feat_file))\n",
        "      dataset_x.append(feat)\n",
        "      dataset_y.append(0)\n",
        "      ce_cnt += 1\n",
        "    except:\n",
        "        print(\"unable to load\")\n",
        "        pass\n",
        "print(\"finished loading %d features for cement test\" % ce_cnt)\n",
        "\n",
        "as_files = os.listdir(as_test_dir)\n",
        "as_cnt = 0\n",
        "print(\"loading test features for asphalt\")\n",
        "for feat_file in tqdm(as_files):\n",
        "  try:\n",
        "    feat = np.load(join(as_test_dir, feat_file))\n",
        "    dataset_x.append(feat)\n",
        "    dataset_y.append(1)\n",
        "    as_cnt += 1\n",
        "  except:\n",
        "    print(\"unable to load\")\n",
        "    pass\n",
        "print(\"finished loading %d features for asphalt test\" % as_cnt)\n",
        "\n",
        "ur_files = os.listdir(ur_test_dir)\n",
        "ur_cnt = 0\n",
        "print(\"loading features for ur test\")\n",
        "for feat_file in tqdm(ur_files):\n",
        "  try:\n",
        "    feat = np.load(join(ur_test_dir, feat_file))\n",
        "    dataset_x.append(feat)\n",
        "    dataset_y.append(2)\n",
        "    ur_cnt += 1\n",
        "  except:\n",
        "    print(\"unable to load\")\n",
        "    pass\n",
        "print(\"finished loading %d features for ur test\" % ur_cnt)\n",
        "\n",
        "\n",
        "x = np.array(dataset_x)\n",
        "y = np.array(dataset_y)\n",
        "le = LabelEncoder()\n",
        "yy = to_categorical(le.fit_transform(y))\n",
        "\n",
        "__, _x_test, _, _y_test = train_test_split(x,yy,test_size=0.99, random_state = RANDOM_STATE)\n",
        "\n",
        "print(\"len test x: \", len(_x_test))\n",
        "print(\"len test y: \", len(_y_test))\n",
        "_x_test = _x_test.reshape(_x_test.shape[0], num_rows, num_columns, num_channels)\n",
        "\n",
        "np.save('/content/drive/My Drive/Kickboard_Software/_x_test',_x_test)\n",
        "np.save('/content/drive/My Drive/Kickboard_Software/_y_test',_y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 15%|█▍        | 83/570 [00:00<00:00, 824.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loading test features for cement\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 570/570 [00:00<00:00, 822.39it/s]\n",
            "  0%|          | 0/600 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finished loading 570 features for cement test\n",
            "loading test features for asphalt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [02:35<00:00,  3.19it/s]\n",
            " 13%|█▎        | 76/606 [00:00<00:00, 756.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finished loading 600 features for asphalt test\n",
            "loading features for ur test\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 606/606 [02:11<00:00,  3.96it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finished loading 606 features for ur test\n",
            "len test x:  1759\n",
            "len test y:  1759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuw_GHMTx9KI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading data\n",
        "_x_train = np.load('/content/drive/My Drive/Kickboard_Software/_x_train_v2_0131.npy')\n",
        "_x_test = np.load('/content/drive/My Drive/Kickboard_Software/_x_test.npy')\n",
        "_y_train = np.load('/content/drive/My Drive/Kickboard_Software/_y_train_v2_0131.npy')\n",
        "_y_test = np.load('/content/drive/My Drive/Kickboard_Software/_y_test.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-hjdPlcpd2w",
        "colab_type": "code",
        "outputId": "ff5762eb-8929-475f-d4dc-59c2e5ec8ab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train\n",
        "print(num_rows)\n",
        "print(num_columns)\n",
        "model = Cnn8layer(num_rows, num_columns, num_channels, num_labels)\n",
        "\n",
        "num_epochs = 10 \n",
        "num_batch_size = 32 \n",
        "model.train(x_train=_x_train, x_test=_x_test, y_train=_y_train, y_test=_y_test, num_epochs=num_epochs, num_batch_size=num_batch_size, checkpoint = checkpointer)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128\n",
            "20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 127, 19, 32)       160       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 127, 19, 32)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 126, 18, 32)       4128      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 63, 9, 32)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 63, 9, 32)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 62, 8, 64)         8256      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 62, 8, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 61, 7, 64)         16448     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 61, 7, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 60, 6, 64)         16448     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 60, 6, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 59, 5, 128)        32896     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 59, 5, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 58, 4, 128)        65664     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 58, 4, 128)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 144,387\n",
            "Trainable params: 144,387\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1759/1759 [==============================] - 14s 8ms/step\n",
            "Pre-training accuracy: 34.5082%\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 17826 samples, validate on 1759 samples\n",
            "Epoch 1/10\n",
            "17826/17826 [==============================] - 6s 355us/step - loss: 0.9914 - acc: 0.5433 - val_loss: 0.8217 - val_acc: 0.6168\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/makinter_checkpoint/8layer_tri_v2_0131.hdf5\n",
            "Epoch 2/10\n",
            "17826/17826 [==============================] - 5s 305us/step - loss: 0.7846 - acc: 0.6694 - val_loss: 0.6565 - val_acc: 0.7322\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/makinter_checkpoint/8layer_tri_v2_0131.hdf5\n",
            "Epoch 3/10\n",
            "17826/17826 [==============================] - 5s 306us/step - loss: 0.6378 - acc: 0.7506 - val_loss: 0.6796 - val_acc: 0.7044\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/makinter_checkpoint/8layer_tri_v2_0131.hdf5\n",
            "Epoch 4/10\n",
            "17826/17826 [==============================] - 5s 305us/step - loss: 0.5910 - acc: 0.7692 - val_loss: 0.5588 - val_acc: 0.7783\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/makinter_checkpoint/8layer_tri_v2_0131.hdf5\n",
            "Epoch 5/10\n",
            "17826/17826 [==============================] - 5s 306us/step - loss: 0.5371 - acc: 0.7981 - val_loss: 0.4483 - val_acc: 0.8232\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/makinter_checkpoint/8layer_tri_v2_0131.hdf5\n",
            "Epoch 6/10\n",
            "17826/17826 [==============================] - 5s 308us/step - loss: 0.4980 - acc: 0.8131 - val_loss: 0.5237 - val_acc: 0.7817\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/makinter_checkpoint/8layer_tri_v2_0131.hdf5\n",
            "Epoch 7/10\n",
            "17826/17826 [==============================] - 5s 303us/step - loss: 0.4746 - acc: 0.8176 - val_loss: 0.5160 - val_acc: 0.7953\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/makinter_checkpoint/8layer_tri_v2_0131.hdf5\n",
            "Epoch 8/10\n",
            "17826/17826 [==============================] - 5s 305us/step - loss: 0.4518 - acc: 0.8292 - val_loss: 0.7331 - val_acc: 0.7038\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/makinter_checkpoint/8layer_tri_v2_0131.hdf5\n",
            "Epoch 9/10\n",
            "17826/17826 [==============================] - 5s 304us/step - loss: 0.4639 - acc: 0.8208 - val_loss: 0.4090 - val_acc: 0.8403\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/makinter_checkpoint/8layer_tri_v2_0131.hdf5\n",
            "Epoch 10/10\n",
            "17826/17826 [==============================] - 5s 305us/step - loss: 0.4172 - acc: 0.8415 - val_loss: 0.3867 - val_acc: 0.8562\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/makinter_checkpoint/8layer_tri_v2_0131.hdf5\n",
            "Training completed in time:  0:00:56.948986\n",
            "Training Accuracy: 0.8736115785930663\n",
            "Testing Accuracy:  0.8561682775320147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcIAeq8oNjr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}